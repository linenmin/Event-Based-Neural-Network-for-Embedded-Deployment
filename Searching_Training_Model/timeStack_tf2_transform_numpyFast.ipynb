{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dfb6a4d-6627-4e6b-8058-117c05a148be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "def robust_fast_read_aedat(filename):\n",
    "    \"\"\"\n",
    "    Skip all header lines starting with '#', then read data as big-endian int32 at once,\n",
    "    and discard the remaining data to ensure reshape(-1,2) is possible.\n",
    "    \"\"\"\n",
    "    # 1. Locate binary section\n",
    "    with open(filename, 'rb') as f:\n",
    "        while True:\n",
    "            pos = f.tell()\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                raise ValueError(f\"No binary data section found in {filename}\")\n",
    "            # Try decoding: if decodable and starts with '#', continue; otherwise it's data section\n",
    "            try:\n",
    "                text = line.decode('ascii')\n",
    "                if text.lstrip().startswith('#'):\n",
    "                    continue\n",
    "            except UnicodeDecodeError:\n",
    "                # Decoding failed, also considered as binary section\n",
    "                pass\n",
    "            # Return to start of this line\n",
    "            f.seek(pos)\n",
    "            break\n",
    "        offset = f.tell()\n",
    "\n",
    "    # 2. Read all int32 data at once\n",
    "    raw = np.fromfile(filename, dtype='>i4', offset=offset)\n",
    "\n",
    "    # 3. Discard extra int32 if length is odd\n",
    "    if raw.size % 2 != 0:\n",
    "        raw = raw[:-1]\n",
    "\n",
    "    # 4. Reshape to separate address and timestamp\n",
    "    events = raw.reshape(-1, 2)\n",
    "    addresses = events[:, 0].astype(np.uint32)\n",
    "    timestamps = events[:, 1]\n",
    "\n",
    "    # 5. Vectorized extraction of x, y, polarity\n",
    "    polarities = (addresses & 1).astype(np.uint8)\n",
    "    xs = ((addresses >> 1) & 0x7F).astype(np.uint8)\n",
    "    ys = ((addresses >> 8) & 0x7F).astype(np.uint8)\n",
    "\n",
    "    return {\n",
    "        'timestamps': timestamps,\n",
    "        'xs': xs,\n",
    "        'ys': ys,\n",
    "        'polarities': polarities\n",
    "    }\n",
    "\n",
    "def read_aedat_file(filename):\n",
    "    \"\"\"Read a single .aedat file and return parsed data\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        header_lines = []\n",
    "        while True:\n",
    "            pos = f.tell()\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                raise ValueError(f\"No binary data section found in file {filename}, please check file format\")\n",
    "\n",
    "            try:\n",
    "                decoded_line = line.decode('ascii', errors='strict')\n",
    "            except UnicodeDecodeError:\n",
    "                # Unable to decode ASCII, means this is the start of binary data\n",
    "                f.seek(pos)\n",
    "                break\n",
    "\n",
    "            stripped_line = decoded_line.strip()\n",
    "            if stripped_line.startswith('#'):\n",
    "                header_lines.append(stripped_line)\n",
    "            else:\n",
    "                f.seek(pos)\n",
    "                break\n",
    "\n",
    "        data_start_index = f.tell()  # Data section start offset\n",
    "        data = f.read()\n",
    "\n",
    "    event_size = 8\n",
    "    num_events = len(data) // event_size\n",
    "\n",
    "    timestamps = []\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for i in range(num_events):\n",
    "        event_data = data[i * event_size:(i + 1) * event_size]\n",
    "        # Parse address and timestamp in big-endian order\n",
    "        address, t = struct.unpack('>ii', event_data)\n",
    "        x = (address >> 1) & 0x7F\n",
    "        y = (address >> 8) & 0x7F\n",
    "\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        timestamps.append(t)\n",
    "\n",
    "    return {\n",
    "        'header': header_lines,\n",
    "        'timestamps': timestamps,\n",
    "        'xs': xs,\n",
    "        'ys': ys\n",
    "    }\n",
    "\n",
    "def process_and_save_event_count_tf(input_base_folder, output_base_folder, grid_size=(128, 128), num_time_bins=1, time_fraction=1):\n",
    "    \"\"\"\n",
    "    Convert aedat files to event count grids and save as TFRecord format.\n",
    "    \n",
    "    Improvements: Use NumPy vectorized processing for event data, use np.histogramdd for fast statistics\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_base_folder):\n",
    "        os.makedirs(output_base_folder)\n",
    "\n",
    "    # Iterate through category folders\n",
    "    class_folders = [f for f in os.listdir(input_base_folder) if os.path.isdir(os.path.join(input_base_folder, f))]\n",
    "    for class_folder in tqdm(class_folders, desc=\"Processing Categories\", unit=\"category\"):\n",
    "        input_folder = os.path.join(input_base_folder, class_folder)\n",
    "        output_folder = os.path.join(output_base_folder, class_folder)\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        # Iterate through .aedat files in current category\n",
    "        files = [f for f in os.listdir(input_folder) if f.endswith('.aedat')]\n",
    "        for filename in tqdm(files, desc=f\"Processing {class_folder}\", unit=\"file\", leave=False):\n",
    "            filepath = os.path.join(input_folder, filename)\n",
    "            output_filepath = os.path.join(output_folder, filename.replace('.aedat', '.tfrecord'))\n",
    "\n",
    "            # Read aedat data\n",
    "            data = robust_fast_read_aedat(filepath)\n",
    "            xs = data['xs']\n",
    "            ys = data['ys']\n",
    "            timestamps = data['timestamps']\n",
    "\n",
    "            # Calculate time window boundaries\n",
    "            t_min, t_max = np.min(timestamps), np.max(timestamps)\n",
    "            # If time_fraction is not 1, only take the first 1/time_fraction of the time range\n",
    "            if time_fraction > 1:\n",
    "                t_max = t_min + (t_max - t_min) / time_fraction\n",
    "\n",
    "            # First filter, only keep events in [t_min, limit]\n",
    "            mask = (timestamps >= t_min) & (timestamps <= t_max)\n",
    "            xs = xs[mask]\n",
    "            ys = ys[mask]\n",
    "            timestamps = timestamps[mask]\n",
    "\n",
    "            # Then bin the data\n",
    "            time_bin_edges = np.linspace(t_min, t_max, num=num_time_bins + 1)\n",
    "\n",
    "            # Vectorized calculation of time indices for all events\n",
    "            t_indices = np.searchsorted(time_bin_edges, timestamps, side='right') - 1\n",
    "\n",
    "            # Construct event coordinate array, shape (N, 3) -> [x, y, t_index]\n",
    "            event_coords = np.stack([xs, ys, t_indices], axis=1)\n",
    "\n",
    "            # Define bin boundaries for each dimension (note: boundaries need one extra)\n",
    "            bins = [np.arange(0, grid_size[0] + 1),\n",
    "                    np.arange(0, grid_size[1] + 1),\n",
    "                    np.arange(0, num_time_bins + 1)]\n",
    "            # Use np.histogramdd to count event distribution, result shape (grid_size[0], grid_size[1], num_time_bins)\n",
    "            event_count_grid, _ = np.histogramdd(event_coords, bins=bins)\n",
    "            event_count_grid = event_count_grid.astype(np.int32)\n",
    "\n",
    "            # If only one time window, copy single channel three times to form 3-channel data\n",
    "            if num_time_bins == 1:\n",
    "                event_count_grid = np.repeat(event_count_grid, 3, axis=-1)\n",
    "\n",
    "            # Convert to TensorFlow tensor (for subsequent serialization)\n",
    "            event_tensor = tf.convert_to_tensor(event_count_grid, dtype=tf.float32)\n",
    "\n",
    "            # Serialize to TFRecord example\n",
    "            serialized_example = serialize_example(event_tensor.numpy())\n",
    "            with tf.io.TFRecordWriter(output_filepath) as writer:\n",
    "                writer.write(serialized_example)\n",
    "\n",
    "def serialize_example(event_grid):\n",
    "    \"\"\"\n",
    "    Serialize event count grid to TFRecord format.\n",
    "    Save two fields:\n",
    "      - 'event_grid': event count grid data stored in bytes format\n",
    "      - 'shape': grid shape information\n",
    "    \"\"\"\n",
    "    feature = {\n",
    "        'event_grid': tf.train.Feature(bytes_list=tf.train.BytesList(value=[event_grid.tobytes()])),\n",
    "        'shape': tf.train.Feature(int64_list=tf.train.Int64List(value=event_grid.shape))\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e450b4-01cd-4a88-827a-37bec5bc0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter settings\n",
    "input_base_folder = r\"C:\\Users\\Lem17\\Master Thesis\\Data processing\\data_aedat2\"\n",
    "output_base_folder = r\"D:\\Dataset\\eventData_dataset\\timeStack_1281281_tf\"\n",
    "num_time_bins = 1  # When set to 1, it will automatically be copied to 3 channels\n",
    "time_fraction = 1\n",
    "grid_size = (128, 128)\n",
    "\n",
    "process_and_save_event_count_tf(input_base_folder, output_base_folder, grid_size=grid_size, num_time_bins=num_time_bins, time_fraction=time_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870f8942-2fcb-4a5b-b36f-1c27edc3cbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Categories: 100%|██████████| 10/10 [03:47<00:00, 22.77s/category]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: D:\\Dataset\\eventData_dataset\\timeStack_1281281_1of1_tf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Categories: 100%|██████████| 10/10 [01:37<00:00,  9.74s/category]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: D:\\Dataset\\eventData_dataset\\timeStack_1281281_1of3_tf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Categories: 100%|██████████| 10/10 [01:34<00:00,  9.41s/category]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: D:\\Dataset\\eventData_dataset\\timeStack_1281281_1of6_tf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Categories: 100%|██████████| 10/10 [01:24<00:00,  8.45s/category]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: D:\\Dataset\\eventData_dataset\\timeStack_1281281_1of12_tf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Categories: 100%|██████████| 10/10 [01:19<00:00,  7.97s/category]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: D:\\Dataset\\eventData_dataset\\timeStack_1281281_1of24_tf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Categories: 100%|██████████| 10/10 [01:15<00:00,  7.59s/category]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: D:\\Dataset\\eventData_dataset\\timeStack_1281281_1of48_tf\n",
      "All processing finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter settings\n",
    "input_base_folder = r\"D:\\Dataset\\eventData_dataset\\data_aedat2\"\n",
    "base_output_folder = r\"D:\\Dataset\\eventData_dataset\"\n",
    "num_time_bins = 1\n",
    "fractions = [1, 3, 6, 12, 24, 48]\n",
    "grid_size = (128, 128)\n",
    "\n",
    "for time_fraction in fractions:\n",
    "    # Construct output folder name\n",
    "    output_folder = f\"{base_output_folder}\\\\timeStack_1281281_1of{time_fraction}_tf\"\n",
    "    # Call main processing function\n",
    "    process_and_save_event_count_tf(\n",
    "        input_base_folder,\n",
    "        output_folder,\n",
    "        grid_size=grid_size,\n",
    "        num_time_bins=num_time_bins,\n",
    "        time_fraction=time_fraction\n",
    "    )\n",
    "    print(f\"Finished: {output_folder}\")\n",
    "\n",
    "print(\"All processing finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
