{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yksIovr6khlY",
        "outputId": "162b5c2a-5987-42c1-f75f-e553a8268d74"
      },
      "outputs": [],
      "source": [
        "# Copy the event camera dataset from Google Drive to current directory\n",
        "!cp /content/drive/MyDrive/Event_Camera_MasterThesis/timeStack_1281281_tf.zip ./\n",
        "# Unzip the dataset to current directory\n",
        "!unzip ./timeStack_1281281_tf.zip -d ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88s87bkDDI17",
        "outputId": "75c32b44-b166-4466-f9e8-1f239c1ecba8"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imdn1uRUkneV"
      },
      "outputs": [],
      "source": [
        "# Create symbolic link to models directory in Google Drive\n",
        "!ln -s /content/drive/MyDrive/Event_Camera_MasterThesis/models models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "-x_Vrf2RlIRd",
        "outputId": "719409b7-fdc2-43fb-f718-a4170abb3553"
      },
      "outputs": [],
      "source": [
        "# Initialize Weights & Biases for experiment tracking\n",
        "import wandb\n",
        "wandb.login()  # Login to wandb account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import TensorFlow and print version for verification\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmX4MOrrlNxt"
      },
      "source": [
        "a5e0804dade6129a32e80434a145699dd5392d03 api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCKFa2SDkok-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from wandb.integration.keras import WandbCallback\n",
        "import wandb\n",
        "from collections import Counter\n",
        "\n",
        "# Custom WandbCallback to skip graph recording\n",
        "class CustomWandbCallback(WandbCallback):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._step = 0\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        if logs is None:\n",
        "            logs = {}\n",
        "        wandb.log(logs, step=self._step)\n",
        "        self._step += 1\n",
        "\n",
        "def normalize_event_grid(event_grid):\n",
        "    # Calculate min and max values for each channel in spatial dimensions\n",
        "    min_vals = tf.reduce_min(event_grid, axis=[0, 1], keepdims=True)\n",
        "    max_vals = tf.reduce_max(event_grid, axis=[0, 1], keepdims=True)\n",
        "    # Avoid division by zero\n",
        "    normalized_grid = (event_grid - min_vals) / (max_vals - min_vals + 1e-8)\n",
        "    return normalized_grid\n",
        "\n",
        "def parse_tfrecord_function(record, filename, table, num_classes):\n",
        "    \"\"\"\n",
        "    Parse single TFRecord and extract label from file path.\n",
        "    Assumes TFRecord contains 'event_grid' and 'shape' fields,\n",
        "    and class name is in second-to-last position of file path.\n",
        "    \"\"\"\n",
        "    features = {\n",
        "        'event_grid': tf.io.FixedLenFeature([], tf.string),\n",
        "        'shape': tf.io.FixedLenFeature([3], tf.int64)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(record, features)\n",
        "    event_grid = tf.io.decode_raw(example['event_grid'], tf.float32)\n",
        "    shape = example['shape']\n",
        "    event_grid = tf.reshape(event_grid, shape)\n",
        "    event_grid = tf.image.resize(event_grid, (224, 224))\n",
        "    event_grid = normalize_event_grid(event_grid)\n",
        "\n",
        "    # Extract label from file path\n",
        "    parts = tf.strings.split(filename, os.sep)\n",
        "    label_str = parts[-2]\n",
        "    label_int = table.lookup(label_str)\n",
        "    label = tf.one_hot(label_int, depth=num_classes)\n",
        "    return event_grid, label\n",
        "\n",
        "def create_dataset_tf2(data_dir, batch_size, seed=42):\n",
        "    \"\"\"\n",
        "    Build TFRecord dataset:\n",
        "    - Get all TFRecord files (excluding .ipynb_checkpoints)\n",
        "    - Perform stratified sampling based on parent folder names\n",
        "    - Read files using tf.data.TFRecordDataset with interleave\n",
        "    - Map to parsing function\n",
        "    \"\"\"\n",
        "    all_files = glob.glob(os.path.join(data_dir, \"*/*.tfrecord\"))\n",
        "    all_files = [f for f in all_files if \".ipynb_checkpoints\" not in f]\n",
        "\n",
        "    # Get valid classes (folder names) and sort\n",
        "    valid_classes = sorted([cls for cls in os.listdir(data_dir)\n",
        "                             if os.path.isdir(os.path.join(data_dir, cls)) and cls != '.ipynb_checkpoints'])\n",
        "    print(\"Valid classes:\", valid_classes)\n",
        "\n",
        "    # Create class lookup table\n",
        "    keys = tf.constant(valid_classes)\n",
        "    vals = tf.constant(range(len(valid_classes)), dtype=tf.int32)\n",
        "    table = tf.lookup.StaticHashTable(\n",
        "        tf.lookup.KeyValueTensorInitializer(keys, vals), default_value=-1)\n",
        "    num_classes = len(valid_classes)\n",
        "\n",
        "    # Get labels for stratified sampling\n",
        "    labels = [os.path.basename(os.path.dirname(f)) for f in all_files]\n",
        "\n",
        "    # Split into train and validation sets\n",
        "    train_files, val_files = train_test_split(\n",
        "        all_files, test_size=0.2, random_state=seed, stratify=labels)\n",
        "\n",
        "    print(\"Training samples:\", len(train_files))\n",
        "    print(\"Validation samples:\", len(val_files))\n",
        "    print(\"Training class distribution:\", Counter([os.path.basename(os.path.dirname(f)) for f in train_files]))\n",
        "    print(\"Validation class distribution:\", Counter([os.path.basename(os.path.dirname(f)) for f in val_files]))\n",
        "\n",
        "    def process_file(filename):\n",
        "        ds = tf.data.TFRecordDataset(filename)\n",
        "        ds = ds.map(lambda record: (record, filename))\n",
        "        return ds\n",
        "\n",
        "    # Build training dataset\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "    train_ds = train_ds.interleave(lambda x: process_file(x),\n",
        "                                   cycle_length=tf.data.AUTOTUNE,\n",
        "                                   block_length=1)\n",
        "    train_ds = train_ds.map(lambda record, filename: parse_tfrecord_function(record, filename, table, num_classes),\n",
        "                            num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    train_ds = train_ds.shuffle(1000, seed=seed).batch(batch_size, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Build validation dataset\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices(val_files)\n",
        "    val_ds = val_ds.interleave(lambda x: process_file(x),\n",
        "                               cycle_length=tf.data.AUTOTUNE,\n",
        "                               block_length=1)\n",
        "    val_ds = val_ds.map(lambda record, filename: parse_tfrecord_function(record, filename, table, num_classes),\n",
        "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    val_ds = val_ds.batch(batch_size, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds\n",
        "\n",
        "def train():\n",
        "    wandb.init()\n",
        "    config = wandb.config\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    seed = 44\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Define data paths for different sampling rates\n",
        "    data_paths = {\n",
        "        \"timeStack1281281_1of1\": \"/content/timeStack_1281281_1of1_tf\",\n",
        "        \"timeStack1281281_1of3\": \"/content/timeStack_1281281_1of3_tf\",\n",
        "        \"timeStack1281281_1of6\": \"/content/timeStack_1281281_1of6_tf\",\n",
        "        \"timeStack1281281_1of12\": \"/content/timeStack_1281281_1of12_tf\",\n",
        "        \"timeStack1281281_1of24\": \"/content/timeStack_1281281_1of24_tf\",\n",
        "        \"timeStack1281281_1of48\": \"/content/timeStack_1281281_1of48_tf\",\n",
        "        \"timeStack1281281_1of96\": \"/content/timeStack_1281281_1of96_tf\"\n",
        "    }\n",
        "    \n",
        "    # Get configuration parameters\n",
        "    data_name = config.get(\"data_name\", \"timeStack1281281\")\n",
        "    learning_rate = config.get(\"learning_rate\", 0.0003)\n",
        "    epochs = config.get(\"epochs\", 90)\n",
        "    batch_size = config.get(\"batch_size\", 16)\n",
        "    patience = config.get(\"patience\", 100)\n",
        "    min_delta = config.get(\"min_delta\", 0.01)\n",
        "    optimizer_name = config.get(\"optimizer\",\"adam\")\n",
        "    l2_reg = config.get(\"dense_l2\", 1e-4)\n",
        "    dropout_rate = config.get(\"dense_dropout\", 0.3)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset, val_dataset = create_dataset_tf2(data_paths[data_name], batch_size, seed)\n",
        "\n",
        "    # Build model architecture\n",
        "    input_tensor = Input(shape=(224, 224, 3))\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "    base_model.trainable = True\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    outputs = Dense(10, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "    # Configure optimizer\n",
        "    if optimizer_name == \"adam\":\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name == \"sgd_0.9\":\n",
        "      optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "    elif optimizer_name == \"sgd_0.95\":\n",
        "      optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.95)\n",
        "    else:\n",
        "      raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    def scheduler(epoch, lr):\n",
        "        if epoch < 20:\n",
        "            return learning_rate * (epoch + 1) / 20  # Linear warmup\n",
        "        elif epoch < 80:\n",
        "            T = 60\n",
        "            cos_inner = np.pi * (epoch - 20) / T\n",
        "            return learning_rate * (np.cos(cos_inner) + 1) / 2  # Cosine decay\n",
        "        else:\n",
        "            return learning_rate * 0.01  # Final small learning rate\n",
        "\n",
        "    # Set up callbacks\n",
        "    lr_callback = LearningRateScheduler(scheduler)\n",
        "    checkpoint_path = f\"/content/models/best{data_name}_{learning_rate}_{batch_size}_{dropout_rate}_{l2_reg}_{optimizer_name}_mbNetV2.h5\"\n",
        "    early_stop = EarlyStopping(monitor='val_accuracy', min_delta=min_delta, patience=patience, restore_best_weights=True)\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "    wandb_callback = CustomWandbCallback(save_model=False, log_graph=False)\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=epochs,\n",
        "        validation_data=val_dataset,\n",
        "        callbacks=[lr_callback, early_stop, checkpoint, wandb_callback]\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    eval_results = model.evaluate(val_dataset)\n",
        "    wandb.log({\"Test Loss\": eval_results[0], \"Test Accuracy\": eval_results[1]*100})\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    valid_classes = sorted([cls for cls in os.listdir(data_paths[data_name])\n",
        "                        if os.path.isdir(os.path.join(data_paths[data_name], cls)) and cls != '.ipynb_checkpoints'])\n",
        "    y_preds = []\n",
        "    y_trues = []\n",
        "    for images, labels in val_dataset:\n",
        "        preds = model.predict(images)\n",
        "        y_preds.extend(np.argmax(preds, axis=1))\n",
        "        y_trues.extend(np.argmax(labels.numpy(), axis=1))\n",
        "    cm = metrics.confusion_matrix(y_trues, y_preds)\n",
        "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=valid_classes)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.savefig(\"confusion_matrix.png\")\n",
        "    wandb.log({\"Confusion Matrix\": wandb.Image(\"confusion_matrix.png\")})\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_lnbYruldp1",
        "outputId": "e1322df9-4661-4254-9d77-92b1d420c1a7"
      },
      "outputs": [],
      "source": [
        "# Step 3: Define sweep configuration for hyperparameter optimization\n",
        "sweep_config = {\n",
        "    \"program\": \"train.py\",  # This doesn't affect the agent running our train() function\n",
        "    \"method\": \"grid\",  # Options: \"grid\", \"random\", \"bayes\" etc.\n",
        "    \"metric\": {\n",
        "        \"name\": \"val_accuracy\",\n",
        "        \"goal\": \"maximize\"  # We want to maximize validation accuracy\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        # Different sampling rates for event camera data\n",
        "        \"data_name\": {\n",
        "            \"values\": [\"timeStack1281281_1of1\", \"timeStack1281281_1of3\", \"timeStack1281281_1of6\",\n",
        "                       \"timeStack1281281_1of12\", \"timeStack1281281_1of24\",\n",
        "                       \"timeStack1281281_1of48\", \"timeStack1281281_1of96\"]\n",
        "        },\n",
        "        # Learning rate configuration\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [3e-3]  # Fixed learning rate of 0.003\n",
        "        },\n",
        "        # Training epochs\n",
        "        \"epochs\": {\n",
        "            \"value\": 100  # Fixed number of epochs\n",
        "        },\n",
        "        # Batch size for training\n",
        "        \"batch_size\": {\n",
        "            \"values\": [16]  # Fixed batch size\n",
        "        },\n",
        "        # Early stopping parameters\n",
        "        \"patience\": {\n",
        "            \"value\": 100  # Number of epochs to wait before early stopping\n",
        "        },\n",
        "        \"min_delta\": {\n",
        "            \"value\": 0.01  # Minimum change in monitored value to qualify as an improvement\n",
        "        },\n",
        "        # Regularization parameters\n",
        "        \"dense_dropout\": {\n",
        "            \"values\": [0.5]  # Dropout rate for dense layers\n",
        "        },\n",
        "        \"dense_l2\": {\n",
        "            \"values\": [1e-3]  # L2 regularization strength\n",
        "        },\n",
        "        # Optimizer selection\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"sgd_0.9\"]  # SGD with momentum of 0.9\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create sweep for hyperparameter optimization\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"event_MT_tf2mobileNetV2_sweep\")\n",
        "print(\"Created sweep with ID:\", sweep_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qxS8rUwUoH6r",
        "outputId": "746757c2-ef70-4498-de6d-72447b2a3b5b"
      },
      "outputs": [],
      "source": [
        "# Step 4：start sweep agent\n",
        "wandb.agent(sweep_id, function=train)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gpu_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
